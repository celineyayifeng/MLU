{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Tabular Data - Lecture 3</a>\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "### Sample Problem: Detecting Products with Electrical Plugs in Amazon Marketplace Products\n",
    "\n",
    "In this notebook, we build, train, validate, and test a Neural Network with PyTorch to predict the __target_label__ field (plug or no plug) of the Amazon electric plug dataset. \n",
    "\n",
    "\n",
    "1. <a href=\"#1\">Read the datasets</a>\n",
    "2. <a href=\"#2\">Data Processing</a>\n",
    "    * <a href=\"#21\">Exploratory Data Analysis</a>\n",
    "    * <a href=\"#22\">Select features to build the model</a>\n",
    "    * <a href=\"#23\">Data Preprocessing (cleaning)</a>\n",
    "    * <a href=\"#24\">Train - Validation - Test Datasets</a>\n",
    "    * <a href=\"#25\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "3. <a href=\"#3\">Neural Network Training and Validation</a>\n",
    "4. <a href=\"#4\">Test the Neural Network</a>\n",
    "5. <a href=\"#5\">Improvement ideas</a>\n",
    "\n",
    "\n",
    "__Dataset schema:__ \n",
    "- __ASIN__: Product ASIN\n",
    "- __target_label:__ Binary field with values in {0,1}. A value of 1 show ASIN has a plug, otherwise 0.\n",
    "- __ASIN_STATIC_ITEM_NAME:__ Title of the ASIN.\n",
    "- __ASIN_STATIC_PRODUCT_DESCRIPTION:__ Description of the ASIN\n",
    "- __ASIN_STATIC_GL_PRODUCT_GROUP_TYPE:__ GL information for the ASIN.\n",
    "- __ASIN_STATIC_ITEM_PACKAGE_WEIGHT:__ Weight of the ASIN.\n",
    "- __ASIN_STATIC_LIST_PRICE:__ Price information for the ASIN.\n",
    "- __ASIN_STATIC_BATTERIES_INCLUDED:__ Information whether batteries are included along with the product.\n",
    "- __ASIN_STATIC_BATTERIES_REQUIRED:__ Information whether batteries are required for using the product.\n",
    "- __ASIN_STATIC_ITEM_CLASSIFICATION:__ Item classification of whether it is a standalone or bundle parent item etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "import boto3\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np                    \n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Read the datasets</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the datasets into dataframes, using [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training dataset is: (55109, 10)\n",
      "The shape of the test dataset is: (6124, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "df = pd.read_csv('../../data/review/asin_electrical_plug_training_data.csv')\n",
    "test_data = pd.read_csv('../../data/review/asin_electrical_plug_test_data.csv')\n",
    "\n",
    "print('The shape of the training dataset is:', df.shape)\n",
    "print('The shape of the test dataset is:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Data Processing</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "### 2.1 <a name=\"21\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We look at number of rows, columns, and some simple statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASIN</th>\n",
       "      <th>target_label</th>\n",
       "      <th>ASIN_STATIC_ITEM_NAME</th>\n",
       "      <th>ASIN_STATIC_PRODUCT_DESCRIPTION</th>\n",
       "      <th>ASIN_STATIC_GL_PRODUCT_GROUP_TYPE</th>\n",
       "      <th>ASIN_STATIC_ITEM_PACKAGE_WEIGHT</th>\n",
       "      <th>ASIN_STATIC_LIST_PRICE</th>\n",
       "      <th>ASIN_STATIC_BATTERIES_INCLUDED</th>\n",
       "      <th>ASIN_STATIC_BATTERIES_REQUIRED</th>\n",
       "      <th>ASIN_STATIC_ITEM_CLASSIFICATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000816IUC</td>\n",
       "      <td>0</td>\n",
       "      <td>Bruder 02921 Jeep Wrangler Unlimited with Hors...</td>\n",
       "      <td>NEW! Jeep Wrangler by Bruder with trailer come...</td>\n",
       "      <td>gl_toy</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>36.66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>base_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B003674A1Y</td>\n",
       "      <td>0</td>\n",
       "      <td>Lucky Reptile OV-2 OpenAir Vivarium, Medium</td>\n",
       "      <td>Größe: 40x40x60 cm. &lt;p&gt;Lucky Reptile OpenAir V...</td>\n",
       "      <td>gl_pet_products</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>24.68</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>base_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B007ECONV4</td>\n",
       "      <td>0</td>\n",
       "      <td>Klarfit KS5DG Chin Up Bar (150kg Max Load, Doo...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;Highly flexible pull-up bar with six con...</td>\n",
       "      <td>gl_sports</td>\n",
       "      <td>8.112928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>base_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00D89465A</td>\n",
       "      <td>0</td>\n",
       "      <td>Liverpool FC Stripe Wallpaper</td>\n",
       "      <td>This fantastic Liverpool Wallpaper is ideal fo...</td>\n",
       "      <td>gl_home</td>\n",
       "      <td>1.763680</td>\n",
       "      <td>8.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B001MJ0BN4</td>\n",
       "      <td>0</td>\n",
       "      <td>Rolson 68889 Oil Tanned Double Tool Pouch</td>\n",
       "      <td>Eleven pockets, two fixed metal hammer holders...</td>\n",
       "      <td>gl_biss</td>\n",
       "      <td>2.733704</td>\n",
       "      <td>27.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ASIN  target_label  \\\n",
       "0  B000816IUC             0   \n",
       "1  B003674A1Y             0   \n",
       "2  B007ECONV4             0   \n",
       "3  B00D89465A             0   \n",
       "4  B001MJ0BN4             0   \n",
       "\n",
       "                               ASIN_STATIC_ITEM_NAME  \\\n",
       "0  Bruder 02921 Jeep Wrangler Unlimited with Hors...   \n",
       "1        Lucky Reptile OV-2 OpenAir Vivarium, Medium   \n",
       "2  Klarfit KS5DG Chin Up Bar (150kg Max Load, Doo...   \n",
       "3                      Liverpool FC Stripe Wallpaper   \n",
       "4          Rolson 68889 Oil Tanned Double Tool Pouch   \n",
       "\n",
       "                     ASIN_STATIC_PRODUCT_DESCRIPTION  \\\n",
       "0  NEW! Jeep Wrangler by Bruder with trailer come...   \n",
       "1  Größe: 40x40x60 cm. <p>Lucky Reptile OpenAir V...   \n",
       "2  <p><b>Highly flexible pull-up bar with six con...   \n",
       "3  This fantastic Liverpool Wallpaper is ideal fo...   \n",
       "4  Eleven pockets, two fixed metal hammer holders...   \n",
       "\n",
       "  ASIN_STATIC_GL_PRODUCT_GROUP_TYPE  ASIN_STATIC_ITEM_PACKAGE_WEIGHT  \\\n",
       "0                            gl_toy                         3.450000   \n",
       "1                   gl_pet_products                         2.690000   \n",
       "2                         gl_sports                         8.112928   \n",
       "3                           gl_home                         1.763680   \n",
       "4                           gl_biss                         2.733704   \n",
       "\n",
       "   ASIN_STATIC_LIST_PRICE ASIN_STATIC_BATTERIES_INCLUDED  \\\n",
       "0                   36.66                          False   \n",
       "1                   24.68                          False   \n",
       "2                     NaN                          False   \n",
       "3                    8.33                            NaN   \n",
       "4                   27.38                            NaN   \n",
       "\n",
       "  ASIN_STATIC_BATTERIES_REQUIRED ASIN_STATIC_ITEM_CLASSIFICATION  \n",
       "0                          False                    base_product  \n",
       "1                          False                    base_product  \n",
       "2                          False                    base_product  \n",
       "3                            NaN                    base_product  \n",
       "4                            NaN                    base_product  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (55109, 10)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55109 entries, 0 to 55108\n",
      "Data columns (total 10 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   ASIN                               55109 non-null  object \n",
      " 1   target_label                       55109 non-null  int64  \n",
      " 2   ASIN_STATIC_ITEM_NAME              55109 non-null  object \n",
      " 3   ASIN_STATIC_PRODUCT_DESCRIPTION    31727 non-null  object \n",
      " 4   ASIN_STATIC_GL_PRODUCT_GROUP_TYPE  55109 non-null  object \n",
      " 5   ASIN_STATIC_ITEM_PACKAGE_WEIGHT    55027 non-null  float64\n",
      " 6   ASIN_STATIC_LIST_PRICE             41182 non-null  float64\n",
      " 7   ASIN_STATIC_BATTERIES_INCLUDED     45016 non-null  object \n",
      " 8   ASIN_STATIC_BATTERIES_REQUIRED     40688 non-null  object \n",
      " 9   ASIN_STATIC_ITEM_CLASSIFICATION    55097 non-null  object \n",
      "dtypes: float64(2), int64(1), object(7)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_label</th>\n",
       "      <th>ASIN_STATIC_ITEM_PACKAGE_WEIGHT</th>\n",
       "      <th>ASIN_STATIC_LIST_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>55109.000000</td>\n",
       "      <td>55027.000000</td>\n",
       "      <td>4.118200e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.036618</td>\n",
       "      <td>31.130529</td>\n",
       "      <td>1.563334e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.187825</td>\n",
       "      <td>458.771422</td>\n",
       "      <td>1.754345e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.160000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>1.667000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.513244</td>\n",
       "      <td>3.268500e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.555592</td>\n",
       "      <td>6.999000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>29500.000000</td>\n",
       "      <td>3.560000e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target_label  ASIN_STATIC_ITEM_PACKAGE_WEIGHT  ASIN_STATIC_LIST_PRICE\n",
       "count  55109.000000                     55027.000000            4.118200e+04\n",
       "mean       0.036618                        31.130529            1.563334e+02\n",
       "std        0.187825                       458.771422            1.754345e+04\n",
       "min        0.000000                         0.000000            1.160000e+00\n",
       "25%        0.000000                         1.520000            1.667000e+01\n",
       "50%        0.000000                         2.513244            3.268500e+01\n",
       "75%        0.000000                         5.555592            6.999000e+01\n",
       "max        1.000000                     29500.000000            3.560000e+06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This prints basic statistics for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution\n",
    "\n",
    "Let's check our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['target_label'].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we are dealing with an imbalanced dataset. This means one result type is dominating the other one(s). In this case, we have a lot of class 0 (\"no plug\") records and very few class 1 (\"plug\") records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset features\n",
    "\n",
    "Let's now print the features of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dataset columns:\n",
      "['ASIN' 'target_label' 'ASIN_STATIC_ITEM_NAME'\n",
      " 'ASIN_STATIC_PRODUCT_DESCRIPTION' 'ASIN_STATIC_GL_PRODUCT_GROUP_TYPE'\n",
      " 'ASIN_STATIC_ITEM_PACKAGE_WEIGHT' 'ASIN_STATIC_LIST_PRICE'\n",
      " 'ASIN_STATIC_BATTERIES_INCLUDED' 'ASIN_STATIC_BATTERIES_REQUIRED'\n",
      " 'ASIN_STATIC_ITEM_CLASSIFICATION']\n",
      "Numerical columns:\n",
      "['target_label' 'ASIN_STATIC_ITEM_PACKAGE_WEIGHT' 'ASIN_STATIC_LIST_PRICE']\n",
      "Categorical columns:\n",
      "['ASIN' 'ASIN_STATIC_ITEM_NAME' 'ASIN_STATIC_PRODUCT_DESCRIPTION'\n",
      " 'ASIN_STATIC_GL_PRODUCT_GROUP_TYPE' 'ASIN_STATIC_BATTERIES_INCLUDED'\n",
      " 'ASIN_STATIC_BATTERIES_REQUIRED' 'ASIN_STATIC_ITEM_CLASSIFICATION']\n"
     ]
    }
   ],
   "source": [
    "# use this for datasets with more columns, to print all columns\n",
    "# (beware, if might raise memory errors when trying to print the text features values!)\n",
    "# np.set_printoptions(threshold=np.inf) \n",
    "\n",
    "# This prints the column labels of the dataframe\n",
    "print('All dataset columns:')\n",
    "print(df.columns.values)\n",
    "\n",
    "# This prints the column labels of the features identified as numerical\n",
    "print('Numerical columns:')\n",
    "print(df.select_dtypes(include=np.number).columns.values)\n",
    "\n",
    "# This prints the column labels of the features identified as numerical\n",
    "print('Categorical columns:')\n",
    "print(df.select_dtypes(include='object').columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 <a name=\"22\">Select features to build the model</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "This time we build a model using all features (except __ASIN__). That is, we build a classifier including __numerical, categorical__ and __text__ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "numerical_features = [\"ASIN_STATIC_ITEM_PACKAGE_WEIGHT\",\n",
    "                      \"ASIN_STATIC_LIST_PRICE\"]\n",
    "\n",
    "categorical_features = ['ASIN_STATIC_GL_PRODUCT_GROUP_TYPE',\n",
    "               'ASIN_STATIC_BATTERIES_INCLUDED',\n",
    "               'ASIN_STATIC_BATTERIES_REQUIRED',\n",
    "               'ASIN_STATIC_ITEM_CLASSIFICATION']\n",
    "\n",
    "text_features = ['ASIN_STATIC_ITEM_NAME',\n",
    "                 'ASIN_STATIC_PRODUCT_DESCRIPTION']\n",
    "\n",
    "model_features = numerical_features + categorical_features + text_features\n",
    "model_target = 'target_label'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 <a name=\"23\">Data Preprocessing (Cleaning)</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "Before data processing, we first clean the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning numerical features \n",
    "\n",
    "Let's examine the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-29.501, 2950.0]     54910\n",
      "(2950.0, 5900.0]         50\n",
      "(5900.0, 8850.0]         28\n",
      "(8850.0, 11800.0]        19\n",
      "(11800.0, 14750.0]        8\n",
      "(14750.0, 17700.0]        7\n",
      "(17700.0, 20650.0]        1\n",
      "(20650.0, 23600.0]        0\n",
      "(23600.0, 26550.0]        1\n",
      "(26550.0, 29500.0]        3\n",
      "Name: ASIN_STATIC_ITEM_PACKAGE_WEIGHT, dtype: int64\n",
      "(-3558.84, 356001.044]        41181\n",
      "(356001.044, 712000.928]          0\n",
      "(712000.928, 1068000.812]         0\n",
      "(1068000.812, 1424000.696]        0\n",
      "(1424000.696, 1780000.58]         0\n",
      "(1780000.58, 2136000.464]         0\n",
      "(2136000.464, 2492000.348]        0\n",
      "(2492000.348, 2848000.232]        0\n",
      "(2848000.232, 3204000.116]        0\n",
      "(3204000.116, 3560000.0]          1\n",
      "Name: ASIN_STATIC_LIST_PRICE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(numerical_features)):\n",
    "    print(df[numerical_features[i]].value_counts(bins=10, sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outliers__. We have an outlier data in the last bin of the second numerical feature. We will remove this data point below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df[df[numerical_features[1]] > 3000000])\n",
    "dropIndexes = df[df[numerical_features[1]] > 3000000].index\n",
    "df.drop(dropIndexes , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-13.838999999999999, 1500.944]    41147\n",
       "(1500.944, 3000.728]                  15\n",
       "(3000.728, 4500.512]                   8\n",
       "(4500.512, 6000.296]                   3\n",
       "(6000.296, 7500.08]                    2\n",
       "(7500.08, 8999.864]                    3\n",
       "(8999.864, 10499.648]                  0\n",
       "(10499.648, 11999.432]                 1\n",
       "(11999.432, 13499.216]                 0\n",
       "(13499.216, 14999.0]                   2\n",
       "Name: ASIN_STATIC_LIST_PRICE, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[numerical_features[1]].value_counts(bins=10, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing Numerical Values__. Let's check missing values for these numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN_STATIC_ITEM_PACKAGE_WEIGHT       82\n",
      "ASIN_STATIC_LIST_PRICE             13927\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[numerical_features].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick fix, we will apply mean imputation. This will replace the missing values with the mean value of the corresponding column.\n",
    "\n",
    "__Note on imputation__: The statistically correct way to perform mean/mode imputation before training an ML model is to compute the column-wise means on the training data only, and then use these values to impute missing data in the train, validation, and test sets. So, we'll need to split our training dataset first. Same goes for any other transformations we would like to apply to these numerical features, such as scaling or encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning categorical features \n",
    "\n",
    "Let's also examine the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN_STATIC_GL_PRODUCT_GROUP_TYPE\n",
      "['gl_toy' 'gl_pet_products' 'gl_sports' 'gl_home' 'gl_biss'\n",
      " 'gl_home_improvement' 'gl_baby_product' 'gl_office_product'\n",
      " 'gl_lawn_and_garden' 'gl_musical_instruments' 'gl_camera' 'gl_kitchen'\n",
      " 'gl_automotive' 'gl_electronics' 'gl_personal_care_appliances' 'gl_pc'\n",
      " 'gl_drugstore' 'gl_luggage' 'gl_wireless' 'gl_home_entertainment'\n",
      " 'gl_major_appliances' 'gl_apparel' 'gl_beauty' 'gl_shoes' 'gl_watch'\n",
      " 'gl_video_games' 'gl_book' 'gl_music' 'gl_fresh_ambient']\n",
      "ASIN_STATIC_BATTERIES_INCLUDED\n",
      "[False nan True]\n",
      "ASIN_STATIC_BATTERIES_REQUIRED\n",
      "[False nan True]\n",
      "ASIN_STATIC_ITEM_CLASSIFICATION\n",
      "['base_product' nan 'variation_parent']\n"
     ]
    }
   ],
   "source": [
    "for c in categorical_features:\n",
    "    print(c)\n",
    "    print(df[c].unique()) #value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note on boolean type__: Most categories are strings, except the __nan__s, and the booleans __False__ and __True__. The booleans will raise errors when trying to encode the categoricals with sklearn encoders, none of which accept boolean types. If using pandas get_dummies to one-hot encode the categoricals, there's no need to convert the booleans. However, get_dummies is trickier to use with sklearn's Pipeline and GridSearch. \n",
    "\n",
    "One way to deal with the booleans is to convert them to strings, by using a mask and a map changing only the booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting booleans to strings for a dataframe\n",
    "def convert_bool_to_str(dataframe):\n",
    "    mask = dataframe.applymap(type) != bool\n",
    "    do = {True: 'TRUE', False: 'FALSE'}\n",
    "    return dataframe.where(mask, dataframe.replace(do))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert booleans to strings for training and test datasets\n",
    "df_masked = convert_bool_to_str(df)\n",
    "test_data_masked = convert_bool_to_str(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN_STATIC_GL_PRODUCT_GROUP_TYPE\n",
      "['gl_toy' 'gl_pet_products' 'gl_sports' 'gl_home' 'gl_biss'\n",
      " 'gl_home_improvement' 'gl_baby_product' 'gl_office_product'\n",
      " 'gl_lawn_and_garden' 'gl_musical_instruments' 'gl_camera' 'gl_kitchen'\n",
      " 'gl_automotive' 'gl_electronics' 'gl_personal_care_appliances' 'gl_pc'\n",
      " 'gl_drugstore' 'gl_luggage' 'gl_wireless' 'gl_home_entertainment'\n",
      " 'gl_major_appliances' 'gl_apparel' 'gl_beauty' 'gl_shoes' 'gl_watch'\n",
      " 'gl_video_games' 'gl_book' 'gl_music' 'gl_fresh_ambient']\n",
      "ASIN_STATIC_BATTERIES_INCLUDED\n",
      "['FALSE' nan 'TRUE']\n",
      "ASIN_STATIC_BATTERIES_REQUIRED\n",
      "['FALSE' nan 'TRUE']\n",
      "ASIN_STATIC_ITEM_CLASSIFICATION\n",
      "['base_product' nan 'variation_parent']\n"
     ]
    }
   ],
   "source": [
    "for c in categorical_features:\n",
    "    print(c)\n",
    "    print(df_masked[c].unique()) #value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to handle the booleans is to convert them to strings by changing the type of all categoricals to 'str'. This will also affect the nans, basically performing imputation of the nans with a 'nans' placeholder value! \n",
    "\n",
    "Applying the type conversion to both categoricals and text features, takes care of the nans in the text fields as well. In case other imputations are planned for the categoricals and/or test fields, notice that the masking shown above leaves the nans unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform boolean to string operation on training and test datasets\n",
    "df[categorical_features + text_features] = df[categorical_features + text_features].astype('str')\n",
    "test_data[categorical_features + text_features] = test_data[categorical_features + text_features].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN_STATIC_GL_PRODUCT_GROUP_TYPE\n",
      "['gl_toy' 'gl_pet_products' 'gl_sports' 'gl_home' 'gl_biss'\n",
      " 'gl_home_improvement' 'gl_baby_product' 'gl_office_product'\n",
      " 'gl_lawn_and_garden' 'gl_musical_instruments' 'gl_camera' 'gl_kitchen'\n",
      " 'gl_automotive' 'gl_electronics' 'gl_personal_care_appliances' 'gl_pc'\n",
      " 'gl_drugstore' 'gl_luggage' 'gl_wireless' 'gl_home_entertainment'\n",
      " 'gl_major_appliances' 'gl_apparel' 'gl_beauty' 'gl_shoes' 'gl_watch'\n",
      " 'gl_video_games' 'gl_book' 'gl_music' 'gl_fresh_ambient']\n",
      "ASIN_STATIC_BATTERIES_INCLUDED\n",
      "['False' 'nan' 'True']\n",
      "ASIN_STATIC_BATTERIES_REQUIRED\n",
      "['False' 'nan' 'True']\n",
      "ASIN_STATIC_ITEM_CLASSIFICATION\n",
      "['base_product' 'nan' 'variation_parent']\n"
     ]
    }
   ],
   "source": [
    "for c in categorical_features:\n",
    "    print(c)\n",
    "    print(df[c].unique()) #value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting categoricals into useful numerical features, will also have to wait until after the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text features \n",
    "\n",
    "Also a good idea to look at the text fields. Text cleaning can be performed here, before train/validation split, with less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN_STATIC_ITEM_NAME\n",
      "['Bruder 02921 Jeep Wrangler Unlimited with Horse Trailer incl. 1 Horse'\n",
      " 'Lucky Reptile OV-2 OpenAir Vivarium, Medium'\n",
      " 'Klarfit KS5DG Chin Up Bar (150kg Max Load, Door Mounted & Steel Frame) - Black'\n",
      " ...\n",
      " 'Hama 00096150 file storage box/organizer - file storage boxes & organizers (177 x 225 x 185 mm)'\n",
      " 'Vogels EFW 8125 MOTION S Wall Mount with Arm for 19 - 32-Inch LCD/Plasma TV'\n",
      " 'Rexel Nyrex Slimview Display Book A4 Black (50 Pockets)']\n",
      "ASIN_STATIC_PRODUCT_DESCRIPTION\n",
      "['NEW! Jeep Wrangler by Bruder with trailer comes with a horse - which makes it a great gift for the equestrian in your family as well as for collectors of unique toys. This well designed combination urges children to use their imagination in constructive play. The horse trailer has a stabilizer leg, a side door that opens and is compatible with Pro Series Tractors and Unimogs. Dimensions: L650 x W140 x H190mm.'\n",
      " 'Größe: 40x40x60 cm. <p>Lucky Reptile OpenAir Vivarium</p> <p>Das OpenAir Vivarium besteht vollkommen aus stabiler Nylongaze. Es eignet sich damit hervorragend für Tiere, die eine gute Belüftung brauchen, wie z.B. Chamäleons. Das OpenAir Vivarium ist auch sehr gut geeignet, um Ihren Tieren einen Aufenhalt im Freien zu ermöglichen; es ist UV durchlässig und die gute Belüftung verringert die berhitzungsgefahr.<br> Das Terrarium lässt sich einfach und schnell auf- und abbauen und ist zerlegt klein und handlich. Damit eignet es sich auch sehr gut für Feldexkursionen. Zum Hängen oder für das Anbringen von Trageriemen sind sen vorhanden.<br> Passende wasserdichte Plastikbodenwannen sind als Zubehör erhältlich.</p> <br><b> 40x40x60 cm'\n",
      " \"<p><b>Highly flexible pull-up bar with six control groups for an efficient and<br>cost-effective upper body workout.<br><br>Designed to be mounted above the door frame. Supplied with two brackets and includes<br>mounting hardware.</b><p>The <b>Klarfit </b>is a classic pull-up bar that's perfect for a quick home work out!<br><br>This sports device allows for an uncomplicated, inexpensive, and highly efficient<br>workout for your chest, back, shoulder, neck and arms.<br><br>Unlike other door bars, the Klarfit pull-up bar provides six handle pairs to<br>individually address several muscle groups.<br><br>The pull up bar comes with 2 hooks to provide security.<br><br>Please note that the unit is shipped as an assembly kit. Construction takes about 10<br>minutes, depending on your construction skills. Mounting hardware is also included<br>with your delivery.<p><b>Top Features:</b><p>• Ultra-sturdy steel frame<br>• Six grips for individual exercises of different muscle groups<br>• Handle grips made of high-density plastic <br>• Slip resistance and joint relief<br><br><p><b>Features:</b><p>• Supplied with mounting hardware<br>• Capacity: up to 150kg<br>• Color: black<br><p><b>Included:</b><p>• Equipment, including installation material<br><p><b>Dimensions:</b><p>• 105 x 22 x40cm (W x H x D)<br>• Weight: 3.5 kg<br>\"\n",
      " ...\n",
      " 'Selbstklebende Etiketten aus hochwertigem Spezialpapier. Auf A4-Blättern. Für alle PC-Drucker und Kopierer, Farblaserdrucker, Farbkopierer und Multifunktionsgeräte. Höchste Verarbeitungssicherheit durch Rundum-Sicherheitskante sowie höchste Schmiegsamkeit. Ideal beschriftbar mit der Etiketten-Software HERMA Label Designer plus. Formate in vielen Softwareprogrammen enthalten. Farbe: weiß., Hersteller: Herma GmbH, Marke: HERMA, Größe (BxH): 70,0x32,0 mm, Inhalt: 2700 Stück/100'\n",
      " 'Port vintages are declared individually by each producer, depending on the year. After a relatively short time in oak the port is transferred to age in bottles, whose labels indicate the year of its harvest. Due to its high levels of concentration, residual sugars and alcohol, vintage port is able to preserve its freshness and fruit over many decades. This glass helps to bring out the classic aromas of vintage port: blackcurrant, pepper, truffles and smoky notes. These aromas can sometimes be masked by the pungency of port&#39;s high alcoholic content - an effect that the small, slender shape of the glass succeeds in avoiding. All the luscious, fresh red fruit is brought to the fore, with acidity and tannins blending together in sweet harmony in the mouth. A pleasantly lingering aftertaste rounds off the delightful experience of drinking a good vintage port from this fine tuned instrument. Recommended for: Vintage Port. Crystal, mouth blown glass, dishwasher proof Volume: 250 ml Height: 17.2 cm Diameter: 6.6 cm Riedel article number (short): 4400/60 Riedel article number (full): 440000060 , Pack contains:1 Riedel Sommeliers Vintage Port, Wine Glass, Accessories for Port, 250ml, 4400/60'\n",
      " 'Forged from chrome vanadium steel hardened, tempered and chrome plated for corrosion protection. Polished Draper HI-TORQ ring end offset at 15&#176;. Jaw offset at 15&#176;. Display carton. Standards: Manufactured in accordance with DIN3113 Contents: 8, 9, 10, 11, 13, 14, 17 and 19mm Brand: Draper. Product Code: 27511']\n"
     ]
    }
   ],
   "source": [
    "for c in text_features:\n",
    "    print(c)\n",
    "    print(df[c].unique()) #value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-use the helper functions from the 'Text processing' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaning functions\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preProcessText(text):\n",
    "    # lowercase and strip leading/trailing white space\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # remove extra white space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning__: The text cleaning process can take a long time to complete, depending on the size of the text data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning:  ASIN_STATIC_ITEM_NAME\n",
      "Text cleaning:  ASIN_STATIC_PRODUCT_DESCRIPTION\n"
     ]
    }
   ],
   "source": [
    "# Clean the text features\n",
    "for c in text_features:\n",
    "    print('Text cleaning: ', c)\n",
    "    df[c] = [cleanSentence(item, stop_words, stemmer) for item in df[c].values]\n",
    "    test_data[c] = [cleanSentence(item, stop_words, stemmer) for item in test_data[c].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned text features of the training and test dataset are ready to be vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 <a name=\"24\">Train - Validation - Test Datasets</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We have already split the original dataset into two data files: __training__ data file (asin_electrical_plug_training_data.csv) with __90%__ of the samples and the __test__ data file (asin_electrical_plug_test_data.csv) with the remaining __10%__. We further split our training dataset into training and validation subsets using sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Validation - Test Datasets shapes:  (46841, 10) (8267, 10) (6124, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(df, test_size=0.15, shuffle=True, random_state=23)\n",
    "\n",
    "# Print the shapes of the Train - Validation - Test Datasets\n",
    "print('Train - Validation - Test Datasets shapes: ', train_data.shape, val_data.shape, test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target balancing\n",
    "\n",
    "This dataset is imbalanced, as one class dominates the other. One way to address the imbalanced dataset is upsampling the rare the class, to have equal number of samples for each class. \n",
    "\n",
    "__Important note:__ We fix the imbalance only in training set. We shouldn't change the validation and test sets, they should follow the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46841, 10) (8267, 10) (6124, 10)\n",
      "45131\n",
      "1710\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, val_data.shape, test_data.shape)\n",
    "\n",
    "print(sum(train_data[model_target] == 0))\n",
    "print(sum(train_data[model_target] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plug_products = train_data[train_data[model_target] == 1]\n",
    "no_plug_products = train_data[train_data[model_target] == 0]\n",
    "\n",
    "upsampled_plug_products = plug_products.sample(n=len(no_plug_products), replace=True, random_state=42)\n",
    "\n",
    "train_data = pd.concat([no_plug_products, upsampled_plug_products])\n",
    "train_data = shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90262, 10) (8267, 10) (6124, 10)\n",
      "45131\n",
      "45131\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, val_data.shape, test_data.shape)\n",
    "\n",
    "print(sum(train_data[model_target] == 0))\n",
    "print(sum(train_data[model_target] == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 <a name=\"25\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We can use the composite Pipeline of Day 2 to train and tune a neural network in sklearn, using its implementation of neural network __MLPClassifier__. However, sklearn is not a neural network framework, lacking access to large scale optimization techniques with GPU support and more neural network related utility functions. \n",
    " \n",
    "We instead build a neural network with __PyTorch__. While for classic, non-neural algorithms, PyTorch is not particularly useful, using an actual deep learning framework for neural network experimentation provides more flexibility and customization.\n",
    "\n",
    "Choice of model and hosting platform aside, we can still reuse the collective ColumnTransformer from Day 2 to preprocess the data for neural network training, validation and test, ensuring that the transformations learned on the train data are performed accordingly on the training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shapes before processing:  (90262, 8) (8267, 8) (6124, 8)\n",
      "Datasets shapes after processing:  (90262, 239) (8267, 239) (6124, 239)\n"
     ]
    }
   ],
   "source": [
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) # Shown in case is needed, not a must with Decision Trees\n",
    "                                ])\n",
    "                  \n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline([\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Shown in case is needed, no effect here as we already imputed with 'nan' strings\n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore')) # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n",
    "                                ])\n",
    "\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vectorizer_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_processor_1 = Pipeline([\n",
    "    ('text_vectorizer_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_processor = ColumnTransformer([\n",
    "    ('numerical_processing', numerical_processor, numerical_features),\n",
    "    ('categorical_processing', categorical_processor, categorical_features),\n",
    "    ('text_processing_0', text_processor_0, text_features[0]),\n",
    "    ('text_processing_1', text_processor_1, text_features[1])\n",
    "                                    ]) \n",
    "\n",
    "# Visualize the data processing pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "data_processor\n",
    "\n",
    "### DATA PROCESSING ###\n",
    "#######################\n",
    "\n",
    "# Get train data to train the network\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target].values\n",
    "\n",
    "# Get validation data to validate the network \n",
    "X_val = val_data[model_features]\n",
    "y_val = val_data[model_target].values\n",
    "\n",
    "# Get test data to test the network for submission to the leaderboard\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target].values\n",
    "\n",
    "print('Datasets shapes before processing: ', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "X_train = data_processor.fit_transform(X_train).toarray()\n",
    "X_val = data_processor.transform(X_val).toarray()\n",
    "X_test = data_processor.transform(X_test).toarray()\n",
    "\n",
    "print('Datasets shapes after processing: ', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Neural Network Training and Validation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We use Pytorch to build a neural network and fit to the training data. We also use the validation data to check performance at the end of each training iteration.\n",
    "\n",
    "For more details on Pytorch and some starter tutorials, you check this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.043854 Validation_loss 0.000086 Seconds 4.878134\n",
      "Epoch 1. Train_loss 0.042772 Validation_loss 0.000084 Seconds 4.904323\n",
      "Epoch 2. Train_loss 0.042100 Validation_loss 0.000082 Seconds 5.096611\n",
      "Epoch 3. Train_loss 0.041259 Validation_loss 0.000079 Seconds 4.835706\n",
      "Epoch 4. Train_loss 0.040373 Validation_loss 0.000077 Seconds 4.981168\n",
      "Epoch 5. Train_loss 0.039623 Validation_loss 0.000075 Seconds 4.837006\n",
      "Epoch 6. Train_loss 0.038860 Validation_loss 0.000074 Seconds 4.847301\n",
      "Epoch 7. Train_loss 0.038191 Validation_loss 0.000073 Seconds 4.815667\n",
      "Epoch 8. Train_loss 0.037662 Validation_loss 0.000071 Seconds 4.892453\n",
      "Epoch 9. Train_loss 0.037147 Validation_loss 0.000071 Seconds 4.890587\n",
      "Epoch 10. Train_loss 0.036694 Validation_loss 0.000070 Seconds 5.070260\n",
      "Epoch 11. Train_loss 0.036382 Validation_loss 0.000069 Seconds 5.267072\n",
      "Epoch 12. Train_loss 0.035987 Validation_loss 0.000068 Seconds 7.628196\n",
      "Epoch 13. Train_loss 0.035558 Validation_loss 0.000068 Seconds 5.799572\n",
      "Epoch 14. Train_loss 0.035244 Validation_loss 0.000067 Seconds 4.813755\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "######################\n",
    "# Hyper-paramaters of the system\n",
    "batch_size = 16\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Let's our data into Pytorch tensors\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train,\n",
    "                                               y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "######################\n",
    "# Create a simple MultiLayer Perceptron using the Sequential mode - add things in sequence\n",
    "#  with two hidden layers of size 64 and 64 \n",
    "#  some dropouts attached to the hidden layers\n",
    "#  one output layer\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(in_features=239,     # Input shape of 239 is expected\n",
    "              out_features=64),    # Linear layer-1 with 64 units\n",
    "    nn.ReLU(),                     # Tanh activation is applied\n",
    "    nn.Dropout(p=.4),              # Apply random 40% drop-out to layer_1\n",
    "    nn.Linear(64, 64),             # Linear layer-2 with 64 units  \n",
    "    nn.ReLU(),                     # Tanh activation is applied\n",
    "    nn.Dropout(p=.3),              # Apply random 30% drop-out to layer_2\n",
    "    nn.Linear(64, 2)               # Output layer with two units\n",
    ").to(device)               \n",
    "\n",
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(xavier_init_weights)\n",
    "\n",
    "######################\n",
    "# Define the loss function and the trainer\n",
    "\n",
    "# Choose Binary Cross Entropy loss for this classification problem\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Choose Stochastic Gradient Descent - can also experiment with other optimizers\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "######################\n",
    "# Network Training and Validation\n",
    "\n",
    "# Starting the outer epoch loop (epoch = full pass through our dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    training_loss = 0.0\n",
    "    \n",
    "    # Training loop: (with autograd and trainer steps)\n",
    "    # This loop does the training of the neural network\n",
    "    # Weights are updated here\n",
    "    net.train() # Activate training mode (dropouts etc.)\n",
    "    for data, target in train_loader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # forward + backward + optimize\n",
    "        output = net(data)\n",
    "        L = loss(output, target)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        # add batch loss\n",
    "        training_loss += L.item()\n",
    "    \n",
    "    net.eval() # Activate eval mode (don't use dropouts etc.)\n",
    "    # Get validation predictions\n",
    "    val_predictions = net(X_val)\n",
    "    # Calculate the validation loss\n",
    "    val_loss = loss(val_predictions, y_val).item()\n",
    "    \n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(y_train)\n",
    "    val_loss = val_loss / len(y_val)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Epoch %s. Train_loss %f Validation_loss %f Seconds %f\" % \\\n",
    "          (epoch, training_loss, val_loss, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Test the Neural Network</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "And now, let's evaluate the performance of the trained network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.73      0.83      5898\n",
      "           1       0.08      0.63      0.14       226\n",
      "\n",
      "    accuracy                           0.72      6124\n",
      "   macro avg       0.53      0.68      0.49      6124\n",
      "weighted avg       0.95      0.72      0.81      6124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "net.eval() # Activate eval mode (don't use dropouts etc.)\n",
    "\n",
    "# Getting test predictions\n",
    "predictions = net(X_test)\n",
    "\n",
    "# Printing performance on the test data\n",
    "print(classification_report(y_test.cpu().numpy(), predictions.argmax(axis=1).cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Improvement Ideas</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "* Further tune network parameters (architecture, # layers, # hidden neurons, activation functions, weights initialization, dropout, optimizer, learning rate, batch size, # epochs), while closely monitoring the loss function and the accuracy on both training and validation as a function of number of epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
